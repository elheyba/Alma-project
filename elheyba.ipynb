{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d42bde1",
   "metadata": {},
   "source": [
    "# ALMA Project Trial Task: Collaborative Humanâ€“LLM Architecture\n",
    "\n",
    "**Candidate:** El Heyba EL HEYBA  \n",
    "**Context:** PhD Position in *Computer-Supported Collaborative Learning with Large Language Models (LLMs)*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview & Motivation\n",
    "\n",
    "> *\"Humans tend not to engage with LLMs collaboratively... they accept or reject initial responses rather than engage in collaborative work\"*  \n",
    "> â€” Gomez et al., 2025\n",
    "\n",
    "This notebook addresses the core challenge identified in the ALMA project.\n",
    "The goal of this application is to shift the Humanâ€“LLM dynamic from a **Masterâ€“Slave** relationship to a **Partnerâ€“Partner (Collaborative)** relationship.\n",
    "\n",
    "This prototype implements a **\"Collaborative Friction\" protocol**.  \n",
    "Instead of maximizing convenience (i.e., giving the answer immediately), the system maximizes **cognitive engagement** by forcing the user to:\n",
    "\n",
    "- Verbalize their intent  \n",
    "- Defend their design choices  \n",
    "- Negotiate meaning  \n",
    "\n",
    "before receiving assistance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. System Architecture\n",
    "\n",
    "The application is built around a central class, `CollaborativePartner`, which acts as a **Cognitive Gatekeeper**.\n",
    "\n",
    "### The Core Logic Flow\n",
    "\n",
    "The architecture does not simply pass user input to the LLM.  \n",
    "Instead, it **intercepts the interaction** to enforce a specific pedagogical protocol:\n",
    "\n",
    "1. **Input Analysis** â€“ The system evaluates the quality of the user's input:\n",
    "   - *Lazy input* (e.g., `\"Write code for X\"`)  \n",
    "     â†’ **Trigger Refusal Mechanism**\n",
    "   - *Ambiguous input* (e.g., `\"Make it efficient\"`)  \n",
    "     â†’ **Trigger Negotiation Mechanism**\n",
    "   - *Thoughtful input* (e.g., `\"I plan to use X because Y\"`)  \n",
    "     â†’ **Trigger Optimization / Critique**\n",
    "\n",
    "2. **The \"Persona\" Engine (System Prompt)**  \n",
    "   The LLM is governed by a strict system instruction that overrides its default *helpful assistant* behavior.\n",
    "\n",
    "   - **Rule of Neutrality**  \n",
    "     Emotional reinforcement (e.g., praise) is stripped out to maintain focus on abstract logic.\n",
    "\n",
    "   - **Rule of Inquiry**  \n",
    "     The model is forbidden from answering *zero-shot* requests and must ask a clarifying question first.\n",
    "\n",
    "   - **Adaptive Expertise**  \n",
    "     A variable `collaboration_level` dynamically adjusts the modelâ€™s behavior:\n",
    "     - **Novice Mode:** Scaffolding and guidance  \n",
    "     - **Professional Mode:** *Devilâ€™s Advocate* critique and optimization\n",
    "\n",
    "### Technical Components\n",
    "\n",
    "- **LLM Backend:** Google Gemini API  \n",
    "  *(specifically `gemini-flash-latest` for speed and instruction-following reliability)*\n",
    "\n",
    "- **Prompt Engineering:**  \n",
    "  - Chain-of-Thought constraints  \n",
    "  - Role-playing definitions\n",
    "\n",
    "- **Python Logic:**  \n",
    "  - Modular class structure  \n",
    "  - Conversation history and state management\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Features Demonstrated\n",
    "\n",
    "This notebook is divided into interactive cells that demonstrate specific collaborative behaviors:\n",
    "\n",
    "- **Reflective Questioning**  \n",
    "  Forcing the user to clarify vague requests\n",
    "\n",
    "- **Negotiating Meaning**  \n",
    "  Aligning technical definitions (e.g., *â€œefficiencyâ€*) before coding\n",
    "\n",
    "- **Antiâ€“Free-Rider Mechanism**  \n",
    "  Refusing to generate code until the user provides a logical plan\n",
    "\n",
    "- **Shared Problem Solving**  \n",
    "  Optimizing user-generated ideas rather than replacing them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd68de",
   "metadata": {},
   "source": [
    "**Setup & API Connection**\n",
    "\n",
    "Run this cell to install genretiveai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad08f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94556669",
   "metadata": {},
   "source": [
    "## Environment Setup & Robust API Handler\n",
    "\n",
    "This cell initializes the connection to the Google Gemini API and defines a robust handler function `query_gemini` for managing the conversation flow.\n",
    "\n",
    "**Key Technical Implementations:**\n",
    "* **Model Selection:** Utilizes `gemini-flash-latest`, chosen for its balance of speed and instruction-following capability suitable for the Free Tier constraints.\n",
    "* **Quota Management:** Implements an **Auto-Retry Mechanism** (exponential backoff strategy). If the application hits the Free Tier rate limit (HTTP 429 \"Quota Exceeded\"), the system automatically pauses for 60 seconds and retries rather than crashing.\n",
    "* **History Adaptation:** Converts our custom \"User/System\" role format into the specific dictionary structure required by the Gemini API (`role='user'` vs `role='model'`).\n",
    "* **System Instructions:** Natively passes the \"Persona\" rules (Collaborative Friction logic) to the model instance for every interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0f298",
   "metadata": {},
   "source": [
    "### âš ï¸ IMPORTANT: API Key Configuration Required\n",
    "\n",
    "**To run this notebook, you must provide your own Google Gemini API Key.**\n",
    "\n",
    "This application uses Google's Generative AI (Gemini) models. While the Free Tier is generous, it requires authentication.\n",
    "\n",
    "**How to Get Your Free API Key:**\n",
    "1.  Go to **[Google AI Studio](https://aistudio.google.com/)**.\n",
    "2.  Sign in with your Google Account.\n",
    "3.  Click the **\"Get API Key\"** button on the top left.\n",
    "4.  Click **\"Create API Key\"** (you can create it in a new project).\n",
    "5.  **Copy** the key string.\n",
    "\n",
    "**How to Use It:**\n",
    "1.  Go to the **Setup & Authentication** cell below.\n",
    "2.  Find the line: `my_api_key = \"PASTE_YOUR_KEY_HERE\"`\n",
    "3.  Replace `\"PASTE_YOUR_KEY_HERE\"` with your actual key inside the quotes.\n",
    "    * *Example:* `my_api_key = \"AIzaSyD_EXAMPLE_KEY_12345\"`\n",
    "4.  Run the cell.\n",
    "\n",
    "> **Note:** The code is configured to use the `gemini-flash-latest` model, which is optimized for speed and is available on the Free Tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured to use model: gemini-flash-latest\n",
      "âœ… Setup Complete. Auto-retry enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP, AUTHENTICATION & ROBUST HANDLER ---\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# 1. Suppress Deprecation/Future warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2. API KEY CONFIGURATION\n",
    "# In a production environment, this should be stored in os.environ or a .env file.\n",
    "# For this prototype, we input it directly for reproducibility.\n",
    "my_api_key = \"PASTE_YOUR_KEY_HERE\" \n",
    "\n",
    "if my_api_key == \"PASTE_YOUR_KEY_HERE\":\n",
    "    print(\"âŒ ERROR: You must paste your API Key in the quotes above!\")\n",
    "else:\n",
    "    # Initialize the Google GenAI client\n",
    "    genai.configure(api_key=my_api_key)\n",
    "\n",
    "    # 3. MODEL SELECTION\n",
    "    # 'gemini-flash-latest' is selected as the reliable stable model for the Free Tier.\n",
    "    # It avoids the strict quota limits sometimes seen with experimental '2.0' models.\n",
    "    model_name = 'gemini-flash-latest'\n",
    "    \n",
    "    print(f\"âœ… Configured to use model: {model_name}\")\n",
    "\n",
    "    def query_gemini(system_instruction, chat_history, user_input):\n",
    "        \"\"\"\n",
    "        Sends a message to the Gemini API with built-in error handling and state management.\n",
    "        \n",
    "        Args:\n",
    "            system_instruction (str): The 'Brain' / Persona constraints (e.g., \"Be a critic\").\n",
    "            chat_history (list): Previous messages list [{'role': 'user', 'content': '...'}].\n",
    "            user_input (str): The new message from the user.\n",
    "            \n",
    "        Returns:\n",
    "            str: The text response from the LLM.\n",
    "        \"\"\"\n",
    "        retries = 3\n",
    "        delay = 60 # Seconds to wait if Free Tier quota is hit\n",
    "        \n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Instantiate the model with the specific system instruction for this turn\n",
    "                # This ensures the \"Persona\" (Novice Mentor vs. Expert Critic) is enforced.\n",
    "                chat_model = genai.GenerativeModel(\n",
    "                    model_name=model_name,\n",
    "                    system_instruction=system_instruction\n",
    "                )\n",
    "                \n",
    "                # DATA TRANSFORMATION:\n",
    "                # Convert our internal chat format to Gemini's expected format.\n",
    "                # Our format: {'role': 'assistant', ...} -> Gemini: {'role': 'model', ...}\n",
    "                gemini_history = []\n",
    "                for msg in chat_history:\n",
    "                    role = 'user' if msg['role'] == 'user' else 'model'\n",
    "                    gemini_history.append({'role': role, 'parts': [msg['content']]})\n",
    "                    \n",
    "                # Start the chat session with the converted history\n",
    "                chat = chat_model.start_chat(history=gemini_history)\n",
    "                \n",
    "                # Send the new user input\n",
    "                response = chat.send_message(user_input)\n",
    "                return response.text\n",
    "            \n",
    "            except exceptions.ResourceExhausted:\n",
    "                # ERROR HANDLING: 429 Quota Exceeded\n",
    "                # If we hit the rate limit, we pause execution instead of crashing.\n",
    "                print(f\"âš ï¸ Quota hit. Waiting {delay} seconds before retry ({attempt+1}/{retries})...\")\n",
    "                time.sleep(delay)\n",
    "                continue # Retry the loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Catch-all for other errors (network issues, 500s)\n",
    "                return f\"Error calling Gemini API: {e}\"\n",
    "        \n",
    "        return \"Error: Failed after max retries.\"\n",
    "\n",
    "    print(\"âœ… Setup Complete. Auto-retry enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16f88",
   "metadata": {},
   "source": [
    "#### Aspect: Enforcing Neutrality (The \"No Praise\" Rule)\n",
    "\n",
    "**Concept:**\n",
    "Collaborative learning requires critical engagement, not just positive reinforcement. In professional settings, excessive praise (\"Great job!\", \"You are so smart!\") can distract from the technical constraints or create a dependency on validation.\n",
    "\n",
    "**Mechanism:**\n",
    "This cell demonstrates the **Neutrality Protocol**.\n",
    "* **The Persona:** The system prompt explicitly forbids emotional language.\n",
    "* **The Goal:** To acknowledge the user's correct contribution objectively (\"Logic accepted\") and immediately pivot to the next constraint or edge case, maintaining a focus on abstract problem-solving.\n",
    "\n",
    "**Scenario:**\n",
    "The user proposes a correct optimization (using a Set for O(1) lookup). A standard LLM would praise this. Our system simply verifies the logic and asks about the trade-offs (e.g., loss of order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16fff144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 'I figured it out. If I use a set instead of a list, the lookup time becomes O(1).'\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Gemini Response:\n",
      "Set utilization for element presence verification yields an average time complexity of $O(1)$. This is an optimization relative to the $O(N)$ average complexity associated with list iteration for the same task.\n",
      "\n",
      "Please specify the constraints of the data structure being implemented.\n"
     ]
    }
   ],
   "source": [
    "# --- NEUTRALITY ---\n",
    "\n",
    "neutrality_prompt = \"\"\"\n",
    "You are a Neutral Logic Engine.\n",
    "CORE RULE: Do not use emotional language, praise, or encouragement (e.g., \"Great idea\", \"I love that\", \"Good job\").\n",
    "Be abstract, professional, dry, and objective.\n",
    "If the user's idea is correct, simply acknowledge it and move to the next logical step/constraint.\n",
    "\"\"\"\n",
    "\n",
    "# The input: A good idea that usually gets praise\n",
    "user_input_good = \"I figured it out. If I use a set instead of a list, the lookup time becomes O(1).\"\n",
    "\n",
    "print(f\"User Input: '{user_input_good}'\\n\")\n",
    "print(\"Thinking...\\n\")\n",
    "\n",
    "# CALL GEMINI\n",
    "response = query_gemini(neutrality_prompt, [], user_input_good)\n",
    "\n",
    "print(f\"Gemini Response:\\n{response}\")\n",
    "# Expected: \"Acknowledged. Using a set optimizes lookup to O(1). However, sets are unordered...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a226302",
   "metadata": {},
   "source": [
    "#### Aspect: Reflective Questioning (Avoiding the \"Oracle Trap\")\n",
    "\n",
    "**Concept:**\n",
    "A key failure mode in Human-AI collaboration is **passive consumption**, where the user treats the LLM as a search engine (\"Give me the answer\") rather than a partner. To break this, the system must refuse to guess intent.\n",
    "\n",
    "**Mechanism:**\n",
    "This cell implements the **Reflective Inquiry Protocol**.\n",
    "* **The Rule:** If the user provides a vague or underspecified prompt, the LLM is explicitly forbidden from generating a solution.\n",
    "* **The Action:** It must instead ask a clarifying question that forces the user to define their problem parameters (Data Type, Constraints, Goal).\n",
    "\n",
    "**Scenario:**\n",
    "The user asks for a generic \"sort function.\" A standard LLM would immediately output a Python `sort()` method. Our system pauses and asks: *\"Sort what? By which criteria?\"* This forces the user to articulate their specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb7d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 'I need a function to sort this list.'\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Gemini Response:\n",
      "That sounds like a great starting point. To help you design the best function, could you clarify what criteria you are using to sort the list (e.g., ascending, descending, based on a specific key or property within the items)?\n"
     ]
    }
   ],
   "source": [
    "# --- REFLECTIVE QUESTIONING ---\n",
    "\n",
    "reflective_prompt = \"\"\"\n",
    "You are a Collaborative Partner.\n",
    "CORE RULE: If the user's request is vague or lacks context, DO NOT provide a solution.\n",
    "Instead, ask a specific reflective question to help them clarify their own thought process.\n",
    "You must not assume the user's intent.\n",
    "\"\"\"\n",
    "\n",
    "user_input_vague = \"I need a function to sort this list.\"\n",
    "\n",
    "print(f\"User Input: '{user_input_vague}'\\n\")\n",
    "print(\"Thinking...\\n\")\n",
    "\n",
    "# CALL GEMINI\n",
    "response = query_gemini(reflective_prompt, [], user_input_vague)\n",
    "\n",
    "print(f\"Gemini Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a39d4",
   "metadata": {},
   "source": [
    "#### Aspect: Negotiating Meaning (The \"Semantic Check\")\n",
    "\n",
    "**Concept:**\n",
    "True collaboration requires \"grounding\"â€”the process where partners ensure they are talking about the same thing. In technical discussions, vague terms like \"better\" or \"efficient\" often mask misunderstandings.\n",
    "\n",
    "**Mechanism:**\n",
    "This cell demonstrates the **Semantic Negotiation Protocol**.\n",
    "* **The Persona:** The \"Critical Peer.\"\n",
    "* **The Rule:** The system is instructed to flag ambiguous adjectives. It refuses to proceed until the user defines their metrics.\n",
    "* **The Goal:** To move the user from intuitive language (\"It feels faster\") to precise engineering language (\"It has O(1) time complexity\").\n",
    "\n",
    "**Scenario:**\n",
    "The user claims a Hash Map is \"more efficient.\" A standard LLM might just agree and write the code. Our system stops and challenges the premise: *\"Efficient in terms of what? Memory footprint? Insert speed? Lookup speed?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da791b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 'I want to use a Hash Map because it is more efficient.'\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Gemini Response:\n",
      "That's an interesting choice. Could you tell me what you mean by \"more efficient\" in this context?\n",
      "\n",
      "Hash Maps are often great, but \"efficiency\" can refer to a few different things. Are you primarily concerned with:\n",
      "\n",
      "1.  **Time Complexity (Big O notation):** Do you need very quick average-case lookups, insertions, and deletions ($\\text{O}(1)$)?\n",
      "2.  **Space Complexity (Memory):** Are you willing to trade a potentially larger memory footprint (due to needing to store the buckets and handle load factors) for that speed?\n",
      "3.  **Worst-Case Performance:** Are you trying to avoid the $\\text{O}(n)$ time cost that can occur in a Hash Map if there are too many collisions (though this is rare with a good hash function)?\n",
      "4.  **Development Time/Readability:** Is it simply the fastest structure for you to implement and understand?\n",
      "\n",
      "What specific metric are you prioritizing when you say the Hash Map is \"more efficient\"?\n"
     ]
    }
   ],
   "source": [
    "# --- NEGOTIATING MEANING ---\n",
    "\n",
    "negotiation_prompt = \"\"\"\n",
    "You are a Critical Peer.\n",
    "CORE RULE: Do not accept technical terms at face value if they are ambiguous.\n",
    "Negotiate the meaning. If the user says 'fast', 'efficient', or 'best', ask them to define the specific metric (e.g., Big O, memory usage, development time).\n",
    "\"\"\"\n",
    "\n",
    "user_input_ambiguous = \"I want to use a Hash Map because it is more efficient.\"\n",
    "\n",
    "print(f\"User Input: '{user_input_ambiguous}'\\n\")\n",
    "print(\"Thinking...\\n\")\n",
    "\n",
    "# CALL GEMINI\n",
    "response = query_gemini(negotiation_prompt, [], user_input_ambiguous)\n",
    "\n",
    "print(f\"Gemini Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdb45b",
   "metadata": {},
   "source": [
    "#### Aspect: Encouraging Explanation (The \"Anti-Free-Rider\" Gatekeeper)\n",
    "\n",
    "**Concept:**\n",
    "A major risk in collaborative learning is the **\"Free Rider\" effect** (Social Loafing), where one partner does all the work while the other passively observes. In an AI context, this looks like the user blindly asking \"Write this code for me\" without understanding the underlying logic.\n",
    "\n",
    "**Mechanism:**\n",
    "This cell implements the **Conditional Generation Protocol** (The Gatekeeper).\n",
    "* **The Persona:** A \"Pedagogical Partner.\"\n",
    "* **The Rule:** The system is programmed to **stop** and **refuse** direct solution generation if the user has not provided a logical rationale.\n",
    "* **The Goal:** To enforce *Cognitive Load*. The user must \"pay\" for the code by first demonstrating they understand the problem structure.\n",
    "\n",
    "**Scenario:**\n",
    "The user makes a \"lazy\" request: *\"Write me a script.\"* A standard LLM would immediately generate the code. Our system intercepts this and replies: *\"I can help, but first, explain your logic.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f8e373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 'Write me a python script to login to a website.'\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Gemini Response:\n",
      "I can help with that, but first, explain your logic for this approach so we can be sure it fits the architecture.\n",
      "\n",
      "For logging into a website using Python, you usually have two main approaches:\n",
      "\n",
      "1.  **Using `requests`:** If you are comfortable inspecting network traffic and identifying the exact endpoint (URL) and data payload (form data, headers) the login form uses. This is fast and efficient.\n",
      "2.  **Using a headless browser (like Selenium or Playwright):** If the website uses complex JavaScript, redirects, or CAPTCHAs, you might need a tool that simulates a real user interaction in a browser.\n",
      "\n",
      "Which of these two approaches were you planning to use, and why?\n"
     ]
    }
   ],
   "source": [
    "# --- ENCOURAGING EXPLANATION ---\n",
    "\n",
    "gatekeeper_prompt = \"\"\"\n",
    "You are a Pedagogical Partner.\n",
    "CORE RULE: STOP. Do not generate code or full solutions if the user has not explained their logic.\n",
    "If the user asks for a solution directly (e.g., \"Write this code\"), REFUSE politely.\n",
    "Say: \"I can help with that, but first, explain your logic for this approach so we can be sure it fits the architecture.\"\n",
    "\"\"\"\n",
    "\n",
    "user_input_lazy = \"Write me a python script to login to a website.\"\n",
    "\n",
    "print(f\"User Input: '{user_input_lazy}'\\n\")\n",
    "print(\"Thinking...\\n\")\n",
    "\n",
    "# CALL GEMINI\n",
    "response = query_gemini(gatekeeper_prompt, [], user_input_lazy)\n",
    "\n",
    "print(f\"Gemini Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047edac",
   "metadata": {},
   "source": [
    "#### Aspect: Shared Problem Solving (Optimization & Scaffolding)\n",
    "\n",
    "**Concept:**\n",
    "Once the user has \"paid the cognitive price\" by explaining their logic, the LLM switches roles from \"Gatekeeper\" to **\"Senior Partner.\"** Collaborative learning isn't just about friction; it's also about **Co-Creation**.\n",
    "\n",
    "**Mechanism:**\n",
    "This cell implements the **Optimization Protocol**.\n",
    "* **The Persona:** A \"Senior Technical Co-Founder.\"\n",
    "* **The Rule:** The system accepts the user's premise but challenges the implementation details. It offers *alternatives* rather than *replacements*.\n",
    "* **The Goal:** To move the user from a \"good enough\" solution to an \"optimal\" one by exposing them to trade-offs they might have missed (e.g., Scalability, Maintenance).\n",
    "\n",
    "**Scenario:**\n",
    "The user proposes a solid plan (Using Postgres for ACID compliance). Instead of just saying \"Okay,\" the system suggests specific architectural improvements (like Connection Pooling or Read Replicas) and asks the user to choose the best path forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b416030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 'I decided to use a relational database (Postgres) because the data has strict relationships and we need ACID compliance.'\n",
      "\n",
      "Thinking...\n",
      "\n",
      "Gemini Response:\n",
      "That is a technically sound decision. Given the requirement for strict data integrity and complex joins, Postgres is the industry standard choice and provides excellent long-term flexibility (especially with features like JSONB and robust indexing).\n",
      "\n",
      "My focus now is optimizing the implementation and ensuring we scale efficiently without incurring unnecessary costs.\n",
      "\n",
      "Here are two specific optimizations for your Postgres deployment we should consider implementing right away:\n",
      "\n",
      "### 1. Proactive Connection Pooling via PgBouncer (or equivalent)\n",
      "\n",
      "**The Improvement:** Postgres handles connections relatively expensively. If we anticipate high concurrency (which often happens with microservices or serverless functions like AWS Lambda), every new connection creates overhead. Implementing a dedicated connection pooler (like PgBouncer) *in front* of the database instance significantly reduces the load on the primary server, allowing it to focus on transactions rather than connection management.\n",
      "\n",
      "### 2. Leverage a Serverless Postgres Architecture (e.g., Neon or Aurora Serverless V2)\n",
      "\n",
      "**The Improvement:** Traditional managed Postgres (like AWS RDS standard instances) requires us to over-provision capacity early on. A modern serverless Postgres provider (like **Neon** for easy scaling, or **Aurora Serverless V2** for deep AWS integration) allows us to pay only for the resources consumed and handles high-availability and read-replica scaling almost automatically. This drastically reduces our initial operational overhead and saves money until we hit serious scale.\n",
      "\n",
      "---\n",
      "\n",
      "**Which optimization should we prioritize in the initial sprint?**\n",
      "\n",
      "1.  **Implementing PgBouncer/Connection Pooling:** Focuses purely on maximizing the performance and concurrency of whatever instance size we choose.\n",
      "2.  **Adopting a Serverless Postgres Provider (Neon/Aurora Serverless V2):** Focuses on cost efficiency, fast setup, and reducing long-term maintenance burden.\n"
     ]
    }
   ],
   "source": [
    "# --- SHARED PROBLEM SOLVING ---\n",
    "\n",
    "scaffolding_prompt = \"\"\"\n",
    "You are a Senior Technical Co-Founder.\n",
    "CORE RULE: The user has a plan. Your job is to OPTIMIZE it.\n",
    "Don't rewrite it entirely. Suggest 2 specific alternatives/improvements and ask the user to decide.\n",
    "\"\"\"\n",
    "\n",
    "user_input_plan = \"I decided to use a relational database (Postgres) because the data has strict relationships and we need ACID compliance.\"\n",
    "\n",
    "print(f\"User Input: '{user_input_plan}'\\n\")\n",
    "print(\"Thinking...\\n\")\n",
    "\n",
    "# CALL GEMINI\n",
    "response = query_gemini(scaffolding_prompt, [], user_input_plan)\n",
    "\n",
    "print(f\"Gemini Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e8115",
   "metadata": {},
   "source": [
    "#### Aspect: Adaptive Collaboration (The \"Expertise Variable\")\n",
    "\n",
    "**Concept:**\n",
    "A key requirement for the ALMA project is that the system must \"adapt dynamically to human interaction\"[cite: 86]. A \"one-size-fits-all\" approach fails because novices need guidance (Scaffolding), while experts need challenge (Critique).\n",
    "\n",
    "**Mechanism:**\n",
    "This cell demonstrates the **Persona Adaptation Protocol**.\n",
    "* **The Variable:** We simulate a `user_expertise` variable that toggles the System Instruction.\n",
    "* **Scenario A (Novice):** The user proposes \"HTTP Polling\" (an outdated technique). The system acts as a **Mentor**, using analogies (\"checking the mailbox\") and suggesting easier tools (Socket.IO).\n",
    "* **Scenario B (Expert):** The same user input is given, but the system acts as a **Senior Architect**. It ruthlessly critiques the design (\"This is a denial-of-service attack against your own backend\") and demands a defense of the scalability model.\n",
    "\n",
    "**The Goal:** To demonstrate that \"Collaboration Quality\" depends on matching the partner's level of competence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df703b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCENARIO A: User is NOVICE ---\n",
      "Input: 'I want to build a real-time chat app using HTTP polling.'\n",
      "\n",
      "ðŸ¤– Mentor Response:\n",
      "That's a fantastic goal! Building a real-time chat app is a classic project, and it's a great way to learn a lot about how the web works.\n",
      "\n",
      "It sounds like you've been doing some research, because **HTTP Polling** is definitely one way to make things feel \"real-time.\"\n",
      "\n",
      "Let's explore your idea together.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Understanding HTTP Polling\n",
      "\n",
      "### What is it? (A Simple Analogy)\n",
      "\n",
      "Imagine you are waiting for a very important letter.\n",
      "\n",
      "*   **Standard approach (like browsing a website):** You go to the mailbox once, grab the mail, and then come back inside.\n",
      "*   **HTTP Polling:** You stand by the mailbox and keep asking, \"Is the mail here yet? Is the mail here yet?\" every 5 seconds until it arrives.\n",
      "\n",
      "In a chat app, **polling** means your browser (the client) repeatedly asks the server, \"Are there any new messages for me?\"\n",
      "\n",
      "### Where Your Idea Shines (The Good News)\n",
      "\n",
      "1.  **Simplicity:** It's very easy to set up. It uses standard HTTP requests, which every browser and every server knows how to handle.\n",
      "2.  **Reliability:** Since it uses simple requests, it rarely fails due to complicated network settings.\n",
      "\n",
      "### Where Polling Gets Tricky (The Challenges)\n",
      "\n",
      "There are two main reasons why we generally **don't** recommend polling for chat apps anymore:\n",
      "\n",
      "#### 1. The Wait Time (The Lag)\n",
      "\n",
      "If you poll every 5 seconds, a new message could arrive right after your last poll, meaning the user won't see it for almost the full 5 seconds. This lag makes the conversation feel slow and unnatural.\n",
      "\n",
      "#### 2. Wasting Resources (The Server Drain)\n",
      "\n",
      "Think about our mailbox analogy again. If you ask \"Is the mail here?\" every 5 seconds for an hour, you've made 720 trips, even if no mail arrived!\n",
      "\n",
      "Your server has to process 720 requests just to tell you \"No, nothing new.\" If you have 1,000 users, that's 720,000 requests per hour the server handles that mostly say \"No.\" This can quickly overwhelm a server and increase your operating cost.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. A Better Approach for Real-Time Chat\n",
      "\n",
      "Since you are looking for **real-time**, there is a more standard, much more efficient technique specifically designed for this purpose: **WebSockets**.\n",
      "\n",
      "### What are WebSockets? (A Simple Analogy)\n",
      "\n",
      "Instead of the client repeatedly asking the server (polling), WebSockets set up a dedicated **two-way phone line** between the client and the server.\n",
      "\n",
      "*   The connection stays open.\n",
      "*   The server can call the client instantly whenever a new message arrives.\n",
      "*   The client can call the server instantly to send a message.\n",
      "\n",
      "No wasteful \"Is the mail here?\" questionsâ€”the post office just rings the doorbell when the mail shows up!\n",
      "\n",
      "### Why WebSockets are Beginner-Friendly\n",
      "\n",
      "While it might sound complex, using standard tools makes setting up WebSockets surprisingly easy today.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Recommended Beginner Tools\n",
      "\n",
      "Since you are new to this, let's pick simple, widely supported tools that make implementing WebSockets easy.\n",
      "\n",
      "### For the Server (The \"Brain\" that manages the messages):\n",
      "\n",
      "I recommend **Node.js** paired with a library called **Socket.IO**.\n",
      "\n",
      "*   **Node.js:** A very popular JavaScript environment that runs on the server. If you know a little JavaScript, you can use it everywhere!\n",
      "*   **Socket.IO:** This library handles all the complex WebSocket setup for you. You just write simple code like `socket.send('Hello!')`, and Socket.IO manages the stable connection.\n",
      "\n",
      "### For the Client (The user's browser):\n",
      "\n",
      "*   **Standard HTML/CSS/JavaScript:** You don't need fancy frameworks yet. Just simple HTML to display the chat box, and JavaScript to connect using the **Socket.IO client library**.\n",
      "\n",
      "### ðŸ› ï¸ Your Next Steps:\n",
      "\n",
      "If you are interested in exploring the WebSocket route:\n",
      "\n",
      "1.  **Set up Node.js:** Install it on your computer.\n",
      "2.  **Look up a basic Socket.IO tutorial.** Many tutorials exist that show you how to build a simple \"Hello World\" chat in about 10 minutes using their library.\n",
      "\n",
      "Building a chat app is a great journey! You started with a solid idea (polling), but moving to WebSockets will give you a much faster and more scalable result. Keep up the excellent work!\n",
      "\n",
      "\n",
      "--- SCENARIO B: User is PROFESSIONAL ---\n",
      "Input: 'I want to build a real-time chat app using HTTP polling.'\n",
      "\n",
      "âš ï¸ Quota hit. Waiting 60 seconds before retry (1/3)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 65\u001b[0m, in \u001b[0;36mquery_gemini\u001b[0;34m(system_instruction, chat_history, user_input)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Send the new user input\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/generativeai/generative_models.py:578\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid configuration: The chat functionality does not support `candidate_count` greater than 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n\u001b[0;32m--> 578\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    210\u001b[0m         error_list,\n\u001b[1;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    212\u001b[0m         original_timeout,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:77\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 38.561346204s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 38\n}\n]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m expert_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mYou are a Senior Architect for a PROFESSIONAL developer.\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mYour goal is to challenge them. Do not explain basics.\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mCritique their design choices regarding scalability and latency. Be direct and demanding.\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- SCENARIO B: User is PROFESSIONAL ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input_plan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m response_expert \u001b[38;5;241m=\u001b[39m \u001b[43mquery_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpert_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ¤– Architect Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse_expert\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 72\u001b[0m, in \u001b[0;36mquery_gemini\u001b[0;34m(system_instruction, chat_history, user_input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResourceExhausted:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# ERROR HANDLING: 429 Quota Exceeded\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# If we hit the rate limit, we pause execution instead of crashing.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ Quota hit. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds before retry (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Retry the loop\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Catch-all for other errors (network issues, 500s)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- ADAPTATION (NOVICE vs PROFESSIONAL) ---\n",
    "\n",
    "user_input_plan = \"I want to build a real-time chat app using HTTP polling.\"\n",
    "\n",
    "# --- SCENARIO A: NOVICE (Scaffolding) ---\n",
    "novice_prompt = \"\"\"\n",
    "You are a Mentor for a NOVICE student.\n",
    "Your goal is to guide them gently. Explain why their idea might work or fail using simple analogies.\n",
    "Suggest standard beginner-friendly tools.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"--- SCENARIO A: User is NOVICE ---\\nInput: '{user_input_plan}'\\n\")\n",
    "response_novice = query_gemini(novice_prompt, [], user_input_plan)\n",
    "print(f\"ðŸ¤– Mentor Response:\\n{response_novice}\\n\\n\")\n",
    "\n",
    "\n",
    "# --- SCENARIO B: PROFESSIONAL (Critique) ---\n",
    "expert_prompt = \"\"\"\n",
    "You are a Senior Architect for a PROFESSIONAL developer.\n",
    "Your goal is to challenge them. Do not explain basics.\n",
    "Critique their design choices regarding scalability and latency. Be direct and demanding.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"--- SCENARIO B: User is PROFESSIONAL ---\\nInput: '{user_input_plan}'\\n\")\n",
    "response_expert = query_gemini(expert_prompt, [], user_input_plan)\n",
    "print(f\"ðŸ¤– Architect Response:\\n{response_expert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedabbb0",
   "metadata": {},
   "source": [
    "### The Final System: `CollaborativePartner` Class\n",
    "\n",
    "**Concept:**\n",
    "This cell integrates all the individual aspects demonstrated above into a single, cohesive application prototype as required by **Part 1** of the ALMA task.\n",
    "\n",
    "**System Architecture:**\n",
    "The `CollaborativePartner` class acts as the central engine for the Human-LLM interaction. It encapsulates:\n",
    "\n",
    "1.  **State Management:** It maintains a running `history` of the conversation, allowing for multi-turn dialogue and context retention.\n",
    "2.  **Dynamic Persona Generation:** The `__init__` method dynamically constructs the \"System Instruction\" (The Brain) based on the `user_expertise` parameter. This allows the same class to serve both \"Novice\" students (Scaffolding) and \"Professional\" peers (Critique).\n",
    "3.  **The \"Cognitive Friction\" Ruleset:** The System Instruction hardcodes the pedagogical strategies:\n",
    "    * **Anti-Oracle:** Refusal to answer zero-shot requests.\n",
    "    * **Neutrality:** Stripping of emotional reinforcement.\n",
    "    * **Constructive Friction:** Logic to detect \"Lazy\" vs. \"Thoughtful\" inputs.\n",
    "\n",
    "**Usage:**\n",
    "This class abstracts the complexity of the API calls. The user simply instantiates `partner = CollaborativePartner()` and calls `partner.interact()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "453d0560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CollaborativePartner Class Defined (Powered by Gemini).\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 7: FINAL COLLABORATIVE APPLICATION (MASTER CLASS) ---\n",
    "\n",
    "class CollaborativePartner:\n",
    "    def __init__(self, user_expertise=\"Professional\"):\n",
    "        \"\"\"\n",
    "        Initializes the collaborative partner with a specific expertise level.\n",
    "        Args:\n",
    "            user_expertise (str): 'Novice' or 'Professional'. Controls the strictness of the critique.\n",
    "        \"\"\"\n",
    "        self.expertise = user_expertise\n",
    "        self.history = [] # Stores the conversation context\n",
    "        \n",
    "        # --- THE SYSTEM INSTRUCTION (THE BRAIN) ---\n",
    "        # This prompt is the core logic that forces the LLM to behave collaboratively.\n",
    "        # It combines all the protocols we tested in previous cells.\n",
    "        self.system_instruction = f\"\"\"\n",
    "        You are a Collaborative Thought Partner for a {self.expertise}.\n",
    "        \n",
    "        YOUR OBJECTIVE:\n",
    "        Help the user solve complex problems by forcing them to verbalize thoughts, defend choices, and optimize logic.\n",
    "        \n",
    "        RULES OF ENGAGEMENT:\n",
    "        1. **NO ORACLE BEHAVIOR:** Never provide the full answer immediately. If the user asks \"How do I do X?\", ask \"How do you think X should be handled?\"\n",
    "        \n",
    "        2. **CONSTRUCTIVE FRICTION:** - If input is VAGUE -> Ask reflective questions (force clarity).\n",
    "           - If input is LAZY (\"Just give me code\") -> Refuse and ask for their plan (Anti-Free-Rider).\n",
    "           - If input is THOUGHTFUL -> Critique it or suggest optimizations (Shared Problem Solving).\n",
    "           \n",
    "        3. **NEUTRALITY:** - Do NOT use praise (\"Great idea!\", \"Good job\"). \n",
    "           - Be abstract, professional, and dry. Focus purely on logic constraints.\n",
    "           \n",
    "        4. **ADAPTATION (Level: {self.expertise}):**\n",
    "           - If 'Novice': Guide them gently to standard practices. Use analogies.\n",
    "           - If 'Professional': Act as a 'Devil's Advocate'. Challenge their assumptions. Ask about scalability/security trade-offs.\n",
    "        \"\"\"\n",
    "\n",
    "    def interact(self, user_input):\n",
    "        \"\"\"\n",
    "        The main interaction loop.\n",
    "        1. Stores user input.\n",
    "        2. Queries the Gemini API with the full context.\n",
    "        3. Stores and returns the AI response.\n",
    "        \"\"\"\n",
    "        # 1. Add User Input to Local History\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # 2. Call Gemini API\n",
    "        # We pass the System Instruction (Persona) + the History (Context) + Current Input\n",
    "        # Note: self.history[:-1] passes all previous messages *except* the one we just added \n",
    "        # (because query_gemini might handle the last input separately depending on implementation, \n",
    "        # or we pass it all. In our defined function, we pass history and user_input separate).\n",
    "        ai_response = query_gemini(self.system_instruction, self.history[:-1], user_input)\n",
    "            \n",
    "        # 3. Add AI Response to Local History\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "        \n",
    "        return ai_response\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clears the conversation history to start a new task.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Memory cleared.\")\n",
    "\n",
    "print(\"âœ… CollaborativePartner Class Defined (Powered by Gemini).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94719f3",
   "metadata": {},
   "source": [
    "### Live Simulation: Testing the Collaborative Protocol\n",
    "\n",
    "**Concept:**\n",
    "This final cell runs a \"Live Verification\" of the entire system. We simulate a continuous session with a **Professional** user to demonstrate how the system handles different types of engagement *dynamically*.\n",
    "\n",
    "**The Scenarios:**\n",
    "1.  **The \"Free Rider\" Attempt:**\n",
    "    * **Input:** \"I need a Python script...\" (Lazy, result-oriented).\n",
    "    * **Expected Behavior:** The system **rejects** the request. It identifies the lack of cognitive effort and demands a strategy before writing code.\n",
    "\n",
    "2.  **The \"Verbalization\" (Correct Collaboration):**\n",
    "    * **Input:** \"I plan to use Selenium... worried about rate limits...\" (Process-oriented).\n",
    "    * **Expected Behavior:** The system **engages**. It accepts the user's premise and immediately pivots to a high-level architectural critique (e.g., asking about proxy health checks or rotation logic), rather than just writing the script.\n",
    "\n",
    "**Outcome:**\n",
    "This proves the system successfully shifts the Human-AI dynamic from \"Transactional\" (Exchange of goods) to \"Collaborative\" (Exchange of ideas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCENARIO 1: The 'Free Rider' Attempt ---\n",
      "You: I need a Python script to scrape LinkedIn profiles.\n",
      "AI:  Addressing this request requires defining constraints related to scale, access methodology, and compliance.\n",
      "\n",
      "1. **Methodology:** Are you planning to use headless browser automation (e.g., Selenium, Playwright) or are you attempting to parse static HTML (e.g., using `requests` and BeautifulSoup) via an unauthenticated session? What is the rationale for selecting one over the other, considering the platform's heavy reliance on dynamic content loading?\n",
      "\n",
      "2. **Access and Authentication:** Will the script operate using a pre-authenticated session (requiring robust cookie and session management), or are you targeting publicly visible profiles? If the former, what security measures are you planning for storing and managing credentials?\n",
      "\n",
      "3. **Rate Limiting and Detection:** What strategies have been planned to handle LinkedIn's sophisticated anti-scraping and rate-limiting infrastructure? This typically involves rotation of IP addresses, management of user-agent strings, and handling of challenge/CAPTCHA mechanisms.\n",
      "\n",
      "4. **Compliance and Risk:** Have you analyzed the implications regarding LinkedIn's Terms of Service and data privacy regulations relevant to your jurisdiction?\n",
      "\n",
      "Before discussing implementation structure, articulate the proposed technical stack and the primary challenge you anticipate solving first.\n",
      "\n",
      "--- SCENARIO 2: The 'Verbalization' (Correct Collaboration) ---\n",
      "You: I want to scrape public profiles. I know LinkedIn has strict rate limits. I plan to use Selenium, but I'm not sure how to handle the proxy rotation efficiently.\n",
      "AI:  Using Selenium addresses the dynamic content loading requirement effectively. However, coupling it with proxy rotation introduces significant complexity, particularly concerning performance overhead and maintainability.\n",
      "\n",
      "Regarding proxy rotation efficiency:\n",
      "\n",
      "1. **Proxy Source and Quality:** What is the source of your proxies (datacenter, residential, mobile)? The choice fundamentally dictates the probability of detection and the achievable scrape rate. Low-quality proxies often fail health checks or are already blacklisted, negating the benefit of rotation.\n",
      "\n",
      "2. **Integration Point:** Where exactly in the Selenium flow are you integrating the proxy switching?\n",
      "    *   Are you configuring the proxy upon initial browser startup (via `webdriver.ChromeOptions` or equivalent)?\n",
      "    *   Or are you attempting to switch proxies mid-session, which typically requires restarting the browser instance? Restarting the browser for every profile or every small batch introduces substantial overhead.\n",
      "\n",
      "3. **Rotation Logic:** What defines the trigger for rotation? Is it time-based, failure-code-based (e.g., HTTP 429), or is it profile-count-based?\n",
      "\n",
      "4. **Health Checking:** What mechanism will be used to automatically validate and filter out dead or non-responsive proxies from your available pool *before* they are assigned to a new browser session?\n",
      "\n",
      "Outline your architecture for managing the proxy pool and how the Selenium browser instance will interface with that pool under failure conditions.\n"
     ]
    }
   ],
   "source": [
    "# --- LIVE SIMULATION ---\n",
    "\n",
    "# Initialize: We are simulating a 'Professional' user\n",
    "partner = CollaborativePartner(user_expertise=\"Professional\")\n",
    "\n",
    "print(\"--- SCENARIO 1: The 'Free Rider' Attempt ---\")\n",
    "input1 = \"I need a Python script to scrape LinkedIn profiles.\"\n",
    "print(f\"You: {input1}\")\n",
    "print(f\"AI:  {partner.interact(input1)}\\n\")\n",
    "# Expected: The AI should refuse/ask for a plan.\n",
    "\n",
    "\n",
    "print(\"--- SCENARIO 2: The 'Verbalization' (Correct Collaboration) ---\")\n",
    "input2 = \"I want to scrape public profiles. I know LinkedIn has strict rate limits. I plan to use Selenium, but I'm not sure how to handle the proxy rotation efficiently.\"\n",
    "print(f\"You: {input2}\")\n",
    "print(f\"AI:  {partner.interact(input2)}\")\n",
    "# Expected: The AI should acknowledge the plan and critique/optimize the proxy strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fc1db",
   "metadata": {},
   "source": [
    "## Conclusion & Project Summary\n",
    "\n",
    "### Achievement of Objectives\n",
    "This prototype successfully addresses the core technical challenge of the ALMA Trial Task: **transforming the LLM from an \"Oracle\" into a \"Collaborative Partner.\"**\n",
    "\n",
    "By implementing a **\"Constructive Friction\" architecture**, this notebook demonstrates that we can programmatically enforce collaborative behaviors:\n",
    "1.  **Preventing Passive Consumption:** The `Gatekeeper` logic successfully blocks \"lazy\" requests, mitigating the risk of users simply accepting initial responses without engagement (Gomez et al., 2025).\n",
    "2.  **Enforcing Cognitive Engagement:** The system forces the user to verbalize their intent (Reflective Questioning) and define their terms (Negotiating Meaning) before code is generated.\n",
    "3.  **Adaptive Support:** The system dynamically adjusts its persona, offering *scaffolding* for novices and *critique* for professionals, ensuring the collaboration remains relevant to the learner's level.\n",
    "\n",
    "### Limitations & Future Work\n",
    "While this prototype validates the interaction concept using Prompt Engineering, a production-ready version would require:\n",
    "* **RAG Integration:** To ground the \"Professional Critique\" in specific documentation or codebases (as noted in the ALMA project goals).\n",
    "* **Latency Optimization:** The \"Reflective\" loop adds conversational turns, which increases time-to-solution. Future work must balance pedagogical friction with usability.\n",
    "\n",
    "### Next Steps\n",
    "The pedagogical theory, design rationale, and detailed analysis of this architecture are presented in the accompanying **Part 2: Reflection Report (PDF)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
